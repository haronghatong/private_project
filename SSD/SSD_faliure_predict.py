import os
import glob
import json
import numpy as np
import joblib  # ëª¨ë¸ ë¡œë“œìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
import re

# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model_path = "C:/Users/pythonfile/SSD_remain/svm_failure_model.pkl"  # ì˜¬ë ¤ì£¼ì‹  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
if not os.path.exists(model_path):
    print(f"ëª¨ë¸ íŒŒì¼ {model_path}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

# ëª¨ë¸ ë¡œë“œ
model_pipeline = joblib.load(model_path)
print(f"âœ… í•™ìŠµëœ ëª¨ë¸ì´ {model_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë””ìŠ¤í¬ ì •ë³´ íŒŒì‹± í•¨ìˆ˜
def parse_disk_info(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    disk_info = {}
    for line in lines:
        if ':' in line:
            key, value = line.strip().split(':', 1)
            disk_info[key.strip()] = value.strip()
    
    capacity_gb = float(disk_info['total'].replace(' GB', ''))
    install_time = disk_info['install_time']
    read_bytes_gb = float(disk_info['read_bytes'].replace(' GB', ''))
    write_bytes_gb = float(disk_info['write_bytes'].replace(' GB', ''))
    percent = float(disk_info['percent'])
    
    return capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent

# ë””ìŠ¤í¬ ì •ë³´ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_disk_info(capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent):
    capacity_bytes = int(capacity_gb * (1024**3))
    
    try:
        date_match = re.search(r'\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼', install_time)
        if date_match:
            install_date = datetime.strptime(date_match.group(), '%Yë…„ %mì›” %dì¼')
        else:
            raise ValueError(f"Unknown date format: {install_time}")
    except Exception as e:
        raise ValueError(f"Invalid date format in install_time: {install_time}, Error: {str(e)}")
    
    today = datetime.now()
    hours_used = (today - install_date).total_seconds() / 3600
    
    # SMART ê°’ ê³„ì‚°
    smart_9_raw = hours_used
    smart_241_raw = write_bytes_gb * (1024**3)
    smart_242_raw = read_bytes_gb * (1024**3)
    
    # ì¶”ê°€ íŠ¹ì„± ê³„ì‚°
    write_intensity = smart_241_raw / (smart_9_raw + 1)
    read_write_ratio = smart_242_raw / (smart_241_raw + 1)
    usage_rate = percent / 100
    
    # ì‚¬ìš© ê¸°ê°„ ê³„ì‚°
    total_years = smart_9_raw / (24 * 365)  # ì´ ì‚¬ìš© ì—°ë„
    years = int(total_years)  # ì •ìˆ˜ ë¶€ë¶„: ì—°ë„
    months = round((total_years - years) * 12)  # ì†Œìˆ˜ì  ë¶€ë¶„ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ê°œì›”ë¡œ ë³€í™˜
    if months == 12:
        years += 1
        months = 0
    
    return {
        'capacity_bytes': capacity_bytes,
        'smart_9_raw': smart_9_raw,
        'smart_241_raw': smart_241_raw,
        'smart_242_raw': smart_242_raw,
        'write_intensity': write_intensity,
        'read_write_ratio': read_write_ratio,
        'usage_rate': usage_rate,
        'usage_years': f"{years}ë…„ {months}ê°œì›”",
        'install_date': install_date
    }

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict_failure_probability(file_path, model_pipeline):
    try:
        capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent = parse_disk_info(file_path)
        processed_data = preprocess_disk_info(capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent)
        
        input_data = np.array([[
            processed_data['capacity_bytes'],
            processed_data['smart_9_raw'],
            processed_data['smart_241_raw'],
            processed_data['smart_242_raw'],
            processed_data['write_intensity'],
            processed_data['read_write_ratio'],
            processed_data['usage_rate']
        ]])
        
        failure_prob = model_pipeline.predict_proba(input_data)[0][1]
        risk_level = (
            "ë‚®ìŒ" if failure_prob < 0.01 else
            "ì¤‘ê°„" if failure_prob < 0.03 else
            "ë†’ìŒ"
        )
        
        prediction_data = {
            "failure_probability": float(failure_prob),
            "failure_probability_percent": float(failure_prob * 100),
            "risk_level": risk_level,
            "usage_years": processed_data['usage_years'],
            "usage_rate": float(processed_data['usage_rate']),
            "capacity_gb": capacity_gb,
            "install_time": install_time,
            "read_bytes_gb": read_bytes_gb,
            "write_bytes_gb": write_bytes_gb,
            "percent_used": percent
        }
        
        return prediction_data
    
    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
        return None

# ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ì˜ˆì¸¡ ì‹¤í–‰
def run_predictions(input_path, output_path):
    disk_info_files = glob.glob(input_path)

    if not disk_info_files:
        print("disk_info_*.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    results = []
    for file_path in disk_info_files:
        result = predict_failure_probability(file_path, model_pipeline)
        if result:
            results.append({"file": file_path, "prediction_data": result})
            
            # ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
            output_file = os.path.join(output_path, os.path.basename(file_path).replace('.txt', '_failure_prediction.json'))
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì €ì¥
    all_results_file = os.path.join(output_path, "all_failure_predictions.json")
    with open(all_results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ê°€ {all_results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return results

# ì‹¤í–‰ ê²½ë¡œ ì„¤ì •
disk_info_path = "C:/Users/pythonfile/SSD_remain/input/disk_info_*.txt"  # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
output_dir = "C:/Users/pythonfile/SSD_remain/output"  # ì¶œë ¥ íŒŒì¼ ì €ì¥ ê²½ë¡œ

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(output_dir, exist_ok=True)

# ì˜ˆì¸¡ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥
all_results = run_predictions(disk_info_path, output_dir)

# ìµœì¢… ê²°ê³¼ ë°˜í™˜
print("\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼:")
print(json.dumps(all_results, indent=2, ensure_ascii=False))
