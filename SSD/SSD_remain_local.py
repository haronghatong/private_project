import os
import glob
import json
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import ElasticNet
from sklearn.utils import shuffle
import re  # ì •ê·œì‹ ì‚¬ìš©

# 1. í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ë° ëª¨ë¸ í•™ìŠµ
# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
extract_path = "C:/Users/pythonfile/SSD_remain"  # ê²½ë¡œ ë³€ê²½

# CSV íŒŒì¼ ì°¾ê¸°
csv_files = [f for f in os.listdir(extract_path) if f.endswith(".csv")]

if not csv_files:
    print("CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# CSV íŒŒì¼ ì½ê¸°
csv_path = os.path.join(extract_path, csv_files[0])
df = pd.read_csv(csv_path)

# ë°ì´í„° ì„ê¸°
df = shuffle(df)

# ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¤‘ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ
important_columns = [
    'capacity_bytes', 
    'failure',          # ìˆ˜ì •: ì´í›„ ì œê±°í•  ì»¬ëŸ¼
    'smart_9_raw',      # Power-On Hours (ì „ì› ì¼œì§„ ì‹œê°„)
    'smart_241_raw',    # Total LBAs Written
    'smart_242_raw'     # Total LBAs Read
]

# ë°ì´í„° ì „ì²˜ë¦¬
df_filtered = df[important_columns].copy()
df_filtered = df_filtered.fillna(0)  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
df_filtered = df_filtered.astype('float64')  # ë°ì´í„° íƒ€ì… ë³€í™˜

# ëª©í‘œ ë³€ìˆ˜ (ë‚¨ì€ ì‚¬ìš© ê°€ëŠ¥ ì‹œê°„) ìƒì„±
max_usage_time = df_filtered["smart_9_raw"].max()
df_filtered["remaining_lifetime"] = (
    max_usage_time - df_filtered["smart_9_raw"]
) * (1 + np.random.uniform(0.9, 1.1, len(df_filtered)))  # ëœë¤ ìŠ¤ì¼€ì¼ ë²”ìœ„ ì¶•ì†Œ

# ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ (ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€)
df_filtered["smart_241_raw"] += np.random.normal(0, 0.05, len(df_filtered))
df_filtered["smart_242_raw"] += np.random.normal(0, 0.05, len(df_filtered))

# X (ì…ë ¥ ë°ì´í„°), y (ì¶œë ¥ ë¼ë²¨) ì„¤ì •
X = df_filtered.drop(columns=['failure', 'remaining_lifetime'])
y = df_filtered["remaining_lifetime"]

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (ëœë¤ ìƒ˜í”Œë§, 80% í•™ìŠµ, 20% í…ŒìŠ¤íŠ¸)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ElasticNet ëª¨ë¸ í•™ìŠµ
elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.7)
elastic_net.fit(X_train_scaled, y_train)

# 2. ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"\nğŸ“Œ {model_name} ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
    print(f"  - MAE (Mean Absolute Error): {mae:.4f}")
    print(f"  - RMSE (Root Mean Squared Error): {rmse:.4f}")
    print(f"  - RÂ² Score: {r2:.4f}")

# ëª¨ë¸ í‰ê°€
elastic_net_pred = elastic_net.predict(X_test_scaled)
evaluate_model(y_test, elastic_net_pred, "ElasticNet")

# 3. ë””ìŠ¤í¬ ì •ë³´ íŒŒì¼ ì˜ˆì¸¡
# ì˜ˆì¸¡ì„ ì§„í–‰í•  íŒŒì¼ ê²½ë¡œ
disk_info_path = "C:/Users/pythonfile/SSD_remain/disk_info_*.txt"  # ê²½ë¡œ ë³€ê²½
disk_info_files = glob.glob(disk_info_path)

if not disk_info_files:
    print("disk_info_*.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# disk_info_*.txt íŒŒì¼ì—ì„œ í•„ìš”í•œ ì„±ë¶„ ì¶”ì¶œ í•¨ìˆ˜
def parse_disk_info_v2(file_path):
    """disk_info_*.txt íŒŒì¼ì—ì„œ í•„ìš”í•œ ì„±ë¶„ ì¶”ì¶œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    disk_info = {}
    for line in lines:
        if ':' in line:
            key, value = line.strip().split(':', 1)
            disk_info[key.strip()] = value.strip()
    
    # í•„ìš”í•œ ì„±ë¶„ ì¶”ì¶œ
    capacity_gb = float(disk_info['total'].replace(' GB', ''))  # total -> capacity_bytes
    install_time = disk_info['install_time']  # ì„¤ì¹˜ ë‚ ì§œ
    read_bytes_gb = float(disk_info['read_bytes'].replace(' GB', ''))  # read_bytes
    write_bytes_gb = float(disk_info['write_bytes'].replace(' GB', ''))  # write_bytes
    percent = float(disk_info['percent'])  # ì‚¬ìš©ë¥ (%)
    
    return capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent


# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_disk_info_v2(capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent):
    """ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ì „ì²˜ë¦¬"""
    # total -> capacity_bytes (GB -> Bytes ë³€í™˜)
    capacity_bytes = int(capacity_gb * (1024**3))
    
    # install_time (ì„¤ì¹˜ ë‚ ì§œ -> ì‚¬ìš© ì‹œê°„ ê³„ì‚°)
    try:
        # ë‚ ì§œë§Œ ì¶”ì¶œ (ì •ê·œì‹ìœ¼ë¡œ "YYYYë…„ MMì›” DDì¼" íŒ¨í„´ ì°¾ê¸°)
        date_match = re.search(r'\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼', install_time)
        if date_match:
            install_date = datetime.strptime(date_match.group(), '%Yë…„ %mì›” %dì¼')
        else:
            raise ValueError(f"Unknown date format: {install_time}")
    except Exception as e:
        raise ValueError(f"Invalid date format in install_time: {install_time}, Error: {str(e)}")
    
    # í˜„ì¬ ë‚ ì§œì™€ ì„¤ì¹˜ ë‚ ì§œë¥¼ ë¹„êµí•´ ì‚¬ìš© ì‹œê°„ ê³„ì‚°
    today = datetime.now()
    hours_used = (today - install_date).total_seconds() / 3600  # ì‚¬ìš© ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
    
    # read_bytes -> daily usage ê³„ì‚°
    daily_usage = (read_bytes_gb / (hours_used / 24)) * 1024  # í•˜ë£¨ í‰ê·  ì‚¬ìš©ëŸ‰ (MB)
    
    # years_used ê³„ì‚°
    years_used = hours_used / (24 * 365)
    
    return capacity_bytes, hours_used, daily_usage, years_used, install_date, percent


def predict_lifetime_v2(file_path, model, scaler):
    """ë””ìŠ¤í¬ ì •ë³´ íŒŒì¼ì—ì„œ ìˆ˜ëª… ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥"""
    try:
        # 1) íŒŒì¼ì—ì„œ ì„±ë¶„ ì¶”ì¶œ
        capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent = parse_disk_info_v2(file_path)
        
        # 2) ì„±ë¶„ ì „ì²˜ë¦¬
        capacity_bytes, hours_used, daily_usage, years_used, install_date, usage_percent = preprocess_disk_info_v2(
            capacity_gb, install_time, read_bytes_gb, write_bytes_gb, percent
        )
        
        # 3) ëª¨ë¸ ì…ë ¥ ë°ì´í„° êµ¬ì„± (4ê°œì˜ í”¼ì²˜ í¬í•¨)
        input_data = np.array([[capacity_bytes, hours_used, daily_usage, usage_percent]])
        input_scaled = scaler.transform(input_data)  # ìŠ¤ì¼€ì¼ë§
        
        # 4) ì˜ˆì¸¡ ì‹¤í–‰
        predicted_hours = model.predict(input_scaled)[0]  # ì”ì—¬ ìˆ˜ëª… ì˜ˆì¸¡
        
        #ì´ ì˜ˆìƒ ìˆ˜ëª… = ì”ì—¬ ìˆ˜ëª… + ì‚¬ìš© ì‹œê°„
        remaining_lifetime = predicted_hours / (24 * 365) # ì”ì—¬ ìˆ˜ëª… (ë…„ ë‹¨ìœ„)
        total_lifetime = (predicted_hours + hours_used) / (24 * 365)  # ì´ ì˜ˆìƒ ìˆ˜ëª… (ë…„ ë‹¨ìœ„)
        
        # remaining_lifetimeì´ 0 ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ê²½ìš° 0ìœ¼ë¡œ ê³ ì •
        if remaining_lifetime < 0:
            remaining_lifetime = 0
            total_lifetime = hours_used / (24 * 365)

        # JSON ë°ì´í„° ìƒì„±
        prediction_data = {
            "remaining_lifetime": (remaining_lifetime),
            "daily_usage": daily_usage,
            "years_used": years_used,
            "total_lifetime": total_lifetime,
            "manufacture_date": install_date.strftime('%Y-%m-%d'),
            "usage_percent": usage_percent
        }
        
        print(f"\nğŸ“‚ íŒŒì¼: {file_path}")
        print(json.dumps(prediction_data, indent=2, ensure_ascii=False))
        
        return prediction_data
    
    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
        return None


# ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ì˜ˆì¸¡ ì‹¤í–‰
results = []
for file_path in disk_info_files:
    result = predict_lifetime_v2(file_path, elastic_net, scaler)
    if result:
        results.append({"file": file_path, "prediction_data": result})
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = file_path.replace('.txt', '_prediction.json')  # íŒŒì¼ ì´ë¦„ í˜•ì‹ ë³€ê²½
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# # ì „ì²´ ê²°ê³¼ë„ ì €ì¥ (í•„ìš”í•œ ê²½ìš°)
# output_path = "C:/Users/Desktop/prediction_results.json"  # ê²½ë¡œ ë³€ê²½
# with open(output_path, 'w', encoding='utf-8') as f:
#     json.dump(results, f, ensure_ascii=False, indent=2)

print("\nâœ… ëª¨ë“  íŒŒì¼ì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
# print(f"ì „ì²´ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

